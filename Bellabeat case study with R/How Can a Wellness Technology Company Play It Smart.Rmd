---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Ask Phase

## Task Summary
You have been asked to focus on one of Bellabeat’s products and analyze smart device data to gain insight into how consumers are using their smart devices. The insights you discover will then help guide marketing strategy for the company. You will present your analysis to the Bellabeat executive team along with your high-level recommendations for Bellabeat’s marketing strategy.

*Company:* Bellabeat[link](https://bellabeat.com/) is a high-tech manufacturer of health-focused products for *women.*

## Stakeholders
* *Urška Sršen:* Chief Creative Officer & Cofounder - Non-technical - Commissioned the project
* *Sando Mur:* Matematician & Cofounder - Technical
* *Marketing Analytics Team* - Technical

## Products
* *Bellabeat app:* The Bellabeat app provides users with health data related to their activity, sleep, stress, menstrual cycle, and mindfulness habits. The Bellabeat app connects to their line of smart wellness products.
* *Leaf:* Wellness tracker that can be worn as a bracelet, necklace, or clip. The Leaf tracker connects to the Bellabeat app to track activity, sleep, and stress.
* *Time:* This wellness watch combines the timeless look of a classic timepiece with smart technology to track user activity, sleep, and stress.
* *Spring:* This is a water bottle that tracks daily water intake using smart technology to ensure that you are appropriately hydrated throughout the day.
* *Bellabeat membership:* Bellabeat also offers a subscription-based membership program for users. Membership gives users 24/7 access to fully personalized guidance on nutrition, activity, sleep, health and beauty, and mindfulness based on their lifestyle and goals

## Distribution Channels
* *Third-party online retailers*
* *Proprietary eCommerce*

## Marketing Channels
The company has invested in traditional advertising media, such as radio, out-of-home billboards, print, and television, but focuses on digital marketing extensively. Bellabeat invests year-round in Google Search, maintaining active Facebook and Instagram pages, and consistently engages consumers on Twitter. Additionally, Bellabeat runs video ads on Youtube and display ads on the Google Display Network to support campaigns around key marketing dates.

## Key Questions
Sršen asks you to analyze smart device usage data in order to gain insight into how consumers use non-Bellabeat smart devices. She then wants you to select one Bellabeat product to apply these insights to in your presentation.
* What are some trends in smart device usage?
* How could these trends apply to Bellabeat customers?
* How could these trends help influence Bellabeat marketing strategy?

In this study, I will look at the aggregated public data on smart device users' daily habits, identify key trends, relationships, and anomalies, and recommend how they can influence Bellabeat marketing strategy.

# Prepare Phase
To conduct the study, we use the FitBit Fitness Tracker Data[link](https://www.kaggle.com/datasets/arashnic/fitbit) located in Kaggle and collected by Mobius.

*Storage:* Once downloaded from Kaggle, I stored the data locally in my PC and in RStudio Cloud.
*Data Format:* The data is organized in a mix of long and wide formats, depending on the data set, so I will need to decide on a preferred dataset once I have explored the question.

*Credibility and Bias:* 
Let's assess the data with the *ROCCC Framework:*
* *Reliable:* The data sets include only 1 month of data collected in 20216 from 30 participants, so the results of this study should be seen as exploratory findings and nothing more.
* *Original:* The data is original.
* *Comprehensive:* Even if there are many data points, the sample contains the logs form only 30 individuals, so I would suggest a z-test between the mean of the means of the different data points, to see if the results of the research are significant.
* *Current:* The data is dated to 2016, and Smart device consumers habits may have chgange considerably since then, so research from this data should be treated only as an exploratory analysis.
* *Cited:* This data is cited, and was uploaded by Mobius[link](https://www.kaggle.com/arashnic) and generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016.

*Licensing, privacy, and accessibility:* The data is anonymized and publicly available, under the license *"CC0: Public Domain".*

*Integrity:* The data sets contain one month of data, but in some of them, a few day are missing. So I will need to decide whether to interpolate that data or remove the rows. Moreover, I may need to consider other data sets.

*Usefulness:* The data shows the habits of potential Bellabeat customers. However, since Bellabeat has women has their main customers, this anonymized data may not represent the habits of the target market. If possible, I advise to investigate the data coming directly from the users of Bellabeat's products.

# Process Phase
Considering the different tools at my disposal, we are going to use a spreadsheet to deal with small data sets, and SQL or RStudio to clean big data sets.

I will join data sets respectively by day, hour, minute, and seconds on the ID attribute, and start exploring the data from the daily data sets, to narrow down subsequently.

Let's install the packages I will need:
```{r}
install.packages("tidyverse")
install.packages("anytime")
```
Then, let's load the libraries:
```{r}
library(tidyverse)

```
Now, we shall import the data sets. Each data set will be assigned to a handle name starting with the time cadence the data is collected with (i.e., daily, hourly, minute, seconds). This will come handy later when I join different data sets based on the time cadence.
```{r}
daily_activity <- read.csv("dailyActivity_merged.csv")
daily_sleep <- read.csv("sleepDay_merged.csv")
daily_weight_log <- read.csv("weightLogInfo_merged.csv")
seconds_heartrate <- read.csv("heartrate_seconds_merged.csv")
hourly_calories <- read.csv("hourlyCalories_merged.csv")
hourly_intensities <- read.csv("hourlyIntensities_merged.csv")
hourly_steps <- read.csv("hourlySteps_merged.csv")
minute_calories <- read.csv("minuteCaloriesNarrow_merged.csv")
minute_intensities <- read.csv("minuteIntensitiesNarrow_merged.csv")
minute_MET <- read.csv("minuteMETsNarrow_merged.csv")
minute_sleep <- read.csv("minuteSleep_merged.csv")
minute_steps <- read.csv("minuteStepsNarrow_merged.csv")

```

Now that the data sets have been uploaded, it needs to be cleaned. I will start by making sure that the attributes have consistent names throughout the data sets, and subsequently I will contintue with standardizing the date format, since I noticed it sometime starts with "4", and other times with "04". Then I will continue by removing duplicate rows with the same Id/time combination, fill empty cells with "0", except "seconds_heartrate", which will have N/A, since an heartrate of 0 would mean that the subject is not alive. Moreover, I will proceed with working with the daily logs first, to narrow down to minutes afterwards if needed. This also ensure I don't waste computing power. Then, I if I deem it necessary I could even calculate an heart rate minute average, to merge the heartrate_seconds data sets with the minute data sets.

Let's see how the columns of each data set are named.

```{r}
colnames(daily_activity)
colnames(daily_sleep)
colnames(daily_weight_log)
```
I notice that while the "Id" column is consistently named, the date attribute is not, so I want to make sure that each data sents have "Date" as the attribute to indicate the day.

```{r}
daily_activity <- daily_activity %>% 
  rename(Date = ActivityDate)

daily_sleep <- daily_sleep %>% 
  rename(Date = SleepDay)
```

Let's see if the column names have been correctly renamed:
```{r}
colnames(daily_activity)
colnames(daily_sleep)
colnames(daily_weight_log)
```
Fantastic, they have now consistent names for the "Date" column. Now, I have to make sure that each one of them has the date logs in the same formats.

Let's check the first values of the data sets:
```{r}
head(daily_activity)
head(daily_sleep)
head(daily_weight_log)
```
Here I see that in the daily_sleep and daily_weight_log data sets, Date is in a date time format, so I can either choose to convert datetime into a date format, or slip the values into two columns. By looking at a sample of the values, I see that I don't need the time information, so it can be removed.


First of all, let's load the "lubridate" package to manupulate date information.

```{r}
library(lubridate)
library(anytime)
```

Let's make the dates consistent:
```{r}

daily_activity$Date <-anydate(daily_activity$Date)
daily_sleep$Date <- anydate(daily_sleep$Date)
daily_weight_log$Date <- anydate(daily_weight_log$Date)

```

I want to quickly inspect if the data sets display a standardized date format:
```{r}
head(daily_activity)
head(daily_sleep)
head(daily_weight_log)
```
I want to sort the tables also in descending order to see if the last few values are correct.
```{r}

daily_activity_desc <- daily_activity %>% arrange(desc(Date))
daily_sleep_desc <- daily_sleep %>% arrange(desc(Date))
daily_weight_log_desc <- daily_weight_log %>% arrange(desc(Date))

head(daily_activity_desc)
head(daily_sleep_desc)
head(daily_weight_log_desc)
```
By looking at each data set in descending order, we can see date the column Date was correctly sorted and its values are consistent.

It's now time to eliminate duplicate rows that have the same combination of Id-Date. Before doing so, I want also to count the number of rows, to see how many rows get deleted.
```{r}
nrow(daily_activity)
nrow(daily_sleep)
nrow(daily_weight_log)
```

Let's remove any duplicate now:
```{r}
daily_activity_unique <- daily_activity %>% distinct(Id, Date, .keep_all = TRUE)
daily_sleep_unique <- daily_sleep %>% distinct(Id, Date, .keep_all = TRUE)
daily_weight_log_unique <- daily_weight_log %>% distinct(Id, Date, .keep_all = TRUE)

write.csv(daily_activity_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_activity_unique.csv", row.names = FALSE)
write.csv(daily_sleep_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_sleep_unique.csv", row.names = FALSE)
write.csv(daily_weight_log_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_weight_log_unique.csv", row.names = FALSE)
```

Let's count the number of rows and print the first rows to asses the rusults.
```{r}
nrow(daily_activity_unique)
nrow(daily_sleep_unique)
nrow(daily_weight_log_unique)

```
We removed a few duplicates, let's check the first rows of the new data sets:
```{r}
head(daily_activity_unique)
head(daily_sleep_unique)
head(daily_weight_log_unique)
```
Here I see that some subjects recorded their log more frequently than others. If this become a problem, we will deal with it later. Especially when we are calculating the averages, for example, we will accept only days with a minimum number of logs to not bias the results.

Now, we want to substitute any blank cell with a NA the daily_weight_log data set, while substitute with 0 in the other two data sets. I decided to use different criteria because in the first due data sets, a participant may have left a cell empty instead of assigning a 0 to it, for times they didn't had any activity or sleep in. On the other hand, when dealing with weight, we want to assign NA, to be consistent with the data set and because logging a weight of "0" would not be realistic.

I created a function to inspect only the column and rows that have an associated attribute and Id, to avoid wasting any computational power on empty cells outside of the data set.

```{r}
replace_empty_cells_with_0 <- function(dataset) {
  id_column <- dataset[, 1]  # Assuming "Id" is the first column
  
  for (col in colnames(dataset)) {
    if (!is.na(col) && col != "" && !is.na(id_column[1]) && id_column[1] != "") {
      dataset[is.na(dataset[, col]), col] <- 0
    }
  }
  
  return(dataset)
}

# Call the function to replace empty cells in all columns
daily_activity_unique <- replace_empty_cells_with_0(daily_activity_unique)
daily_sleep_unique <- replace_empty_cells_with_0(daily_sleep_unique)

write.csv(daily_activity_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_activity_unique.csv", row.names = FALSE)
write.csv(daily_sleep_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_sleep_unique.csv", row.names = FALSE)
```

I'm now writing any function to fill the empty cells in the daily_weight_log data set, because empty cells are automatically filled with "NA" in RStudio.

Let's check the results to see if any row was deleted by mistake:
```{r}
nrow(daily_activity_unique)
nrow(daily_sleep_unique)
nrow(daily_weight_log_unique)
```
```{r}
print(sum(duplicated(daily_activity_unique)))
print(sum(duplicated(daily_sleep_unique)))
print(sum(duplicated(daily_weight_log_unique)))
```
In a real situation I would have checked the number of duplicated rows before proceeding with eliminating them, but since this is a case study, I wanted to take this extra step anyways.

And the see the first few rows:
```{r}
head(daily_activity_unique)
head(daily_sleep_unique)
head(daily_weight_log_unique)
```
I'm happy with how the data set is being cleaned so far and will pass to transforming the data and visualizing it to better understand its structure now that duplicate rows have been eliminated, the date format has been standardized, and empty cells have been filled. While analyzing the data, I may deem so rows to be useless if certain key attributes contain "0" or "NA" as values. In that case, I will go back to the cleaning phase and delete those rows.

Let's join the three data sets on the Id-Date columns with an outer joint, to create a data set with all the daily data for every subject, so we can visualize it all together.
```{r}
daily_activity_sleep <- merge(daily_activity_unique, daily_sleep_unique, by = c("Id", "Date"), all = TRUE)
daily_activity_sleep_weight <- merge(daily_activity_sleep, daily_weight_log_unique, by = c("Id", "Date"), all = TRUE)
```

Let's see the columns in this new data set:
```{r}
colnames(daily_activity_sleep_weight)
```
Let's see the first 6 rows:
```{r}
head(daily_activity_sleep_weight)
```

Let's count the number of rows to see wether any row was deleted:
```{r}
nrow(daily_activity_sleep_weight)
```
No row was deleted. However, I saw a LogId column I missed earlier, let's see what it contains and how it differs from the Id column.
```{r}
head(daily_activity_sleep_weight[, c("Id", "LogId")])
```

I don't think I'm going to use neither the LogId and Fat columns, so let's drop them. Since the three data sets have a different number of entries. I will have to interpolate some values to fill the empty cells, for example assuming that within the same week, the weight of the subject didn't changed. Even if I could do it right away, I will do it later, because I want to visualize the data first.

Let's drop the LogId and Fat columns:
```{r}
daily_activity_sleep_weight <- daily_activity_sleep_weight[, !(colnames(daily_activity_sleep_weight) %in% c("LogId", "Fat"))]

write.csv(daily_activity_sleep_weight, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/daily_activity_sleep_weight.csv", row.names = FALSE)
```

Let's inspect the column names now:
```{r}
colnames(daily_activity_sleep_weight)
```
Fantastic, I decided also to work with the seconds_heartrate data_set, to see patterns in the daily physical activity. So, let's clean it. Regarding the hourly and minute data sets, I will live them alone for the moment to avoid getting too much into the weed at this stage. If further analysis is needed because of some particular trends in the daily data, they can be analysed subsequently.

Let's inspect the data set:
```{r}
colnames(seconds_heartrate)
```

I see only three columns, let's see the number of rows.
```{r}
nrow(seconds_heartrate)
```
There are quite a few rows, so I will need also to think whether I can process such an amount of data in the designed time frame for the project. Maybe I will calculating and using daily averages to see if people do more physical activity on certain days, but I can use the daily activity log for that. So, let's do some basic cleaning in case I need the data for later.

Let's see the first 6 rows to inspect the time format.
```{r}
head(seconds_heartrate)
```

I want to make sure that the "Time" column uses consistent values:
```{r}
seconds_heartrate$Time <- as.POSIXct(seconds_heartrate$Time, format = "%m/%d/%Y %I:%M:%S %p")
```
Let's inspect the data set:
```{r}
head(seconds_heartrate)
```
```{r}
seconds_heartrate_desc <- seconds_heartrate %>% arrange(desc(Time))

head(seconds_heartrate_desc)
```

The data set looks fine, let's eliminate duplicate rows for the same Id-Time combination:
```{r}
seconds_heartrate_unique <- seconds_heartrate %>% distinct(Id, Time, .keep_all = TRUE)

write.csv(seconds_heartrate_unique, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/seconds_heartrate_unique.csv", row.names = FALSE)
```

Let's count the number of rows to see if we deleted some:
```{r}
nrow(seconds_heartrate_unique)
```
```{r}
#This would make R crash because the data set is too big
#print(sum(duplicated(seconds_heartrate_unique)))
```

#Analyze

Let's conduct some exploratory analysis on the data set.
```{r}
install.packages("DataExplorer")
```
```{r}
library(DataExplorer)
```
I will use the "DataExplorer" library to find potential correlations in the data set through exploratory data analysis (EDA). Even if using acronyms is not best practice, I will use the acronym "EDA" into the report's name for brevity.
```{r}
daily_activity_sleep_weight_eda <- create_report(daily_activity_sleep_weight)
```

You can access the report here[link](https://bca95afddf464c6d846a028aec110914.app.posit.cloud/file_show?path=%2Fcloud%2Fproject%2Freport.html)

I can't see any obvious pattern and compelling correlation by looking at the report, so before thinking of adding even more granular data, and produce even more confusing results, I may want to interpolate the data to fill the missing values in the sleep and weight logs.

However, before doing so, I want to investigate if there is a common pattern in exercise habits between the subjects. If the subjects perform their daily or weekly activity at a consistent time, it would be a relevant piece of information in timing the promotion of our Spring water bottle and potentially other products. What is a best time to promote a water bottle than when the audience is heavily dehydrated.

To do so, I will first make an average of the heartbeat of all the participant for each second. Afterwards, depending on the results, I may compute the average heartbeat of every minute.

```{r}
average_heart_rate <- seconds_heartrate_unique %>%
  group_by(Time) %>%
  summarize(Average_HR = mean(Value))

write.csv(average_heart_rate, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_heart_rate.csv", row.names = FALSE)
```
Let's inspect the first six rows of the data set.
```{r}
head(average_heart_rate)
```

At this point, I have a few options, for example, I can group the data by days of the week so I can see the average heart rate for every second within an average weekly activity. On the other hand, I could group by month first, because the subjects may structure their daily activity differently for each week of the month. But let's start by plotting this data, to spot any potential difference between the months.The data sample may still be too big to plot it, but let's see what it happens.

First of all, let's split the Time column into Date and Time.

```{r}
average_heart_rate$Date <- as.Date(average_heart_rate$Time)
average_heart_rate$Time <- format(average_heart_rate$Time, "%H:%M:%S")
```

```{r}
average_heart_rate <- average_heart_rate[, c("Date", "Time", "Average_HR")]
write.csv(average_heart_rate, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_heart_rate.csv", row.names = FALSE)
head(average_heart_rate)
```


Let's group Average_HR per day of the week to see patterns and in weekly activity.
```{r}
# Extract day of the week from the "Date" column and create "DayOfWeek" column
average_heart_rate <- average_heart_rate %>%
  mutate(DayOfWeek = format(Date, "%A"))

# Calculate average heart rate for each timestamp of each day
average_hr_by_day_seconds <- average_heart_rate %>%
  group_by(DayOfWeek, Time) %>%
  summarize(Average_HR = mean(Average_HR))
```
```{r}
write.csv(average_hr_by_day_seconds, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_hr_by_day_seconds.csv", row.names = FALSE)
head(average_hr_by_day_seconds)
```

Let's count the number of rows to see if it is realistic to create a plot out of it.
```{r}
nrow(average_hr_by_day_seconds)
```
There are still a few to many rows to plot the dataset, and to be realistic, I don't need the data divided in seconds. Having the data grouped by minutes or even hours will do. So let's group it by minutes first.
```{r}
# Convert "Time" column to POSIXct datetime format
average_heart_rate$Time <- as.POSIXct(average_heart_rate$Time, format = "%H:%M:%S")

# Extract minute component from "Time" column and create "Minute" column
average_heart_rate$Minute <- format(average_heart_rate$Time, "%H:%M")

# Extract day of the week from the "Date" column and create "DayOfWeek" column
average_heart_rate$DayOfWeek <- format(average_heart_rate$Date, "%A")

# Calculate average heart rate for each minute of each day of the week
average_hr_by_minute <- average_heart_rate %>%
  group_by(DayOfWeek, Date, Minute) %>%
  summarize(Average_HR = mean(Average_HR))
```
```{r}
write.csv(average_hr_by_minute, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_hr_by_day_seconds.csv", row.names = FALSE)

head(average_hr_by_minute)
```

```{r}
# Calculate average heart rate for all DayOfWeek - Minute combinations
average_hr_by_day_minute_DayOfWeek <- average_hr_by_minute %>%
  group_by(DayOfWeek, Minute) %>%
  summarize(Average_HR = mean(Average_HR))
```
```{r}
write.csv(average_hr_by_day_minute_DayOfWeek, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_hr_by_day_minute_DayOfWeek.csv", row.names = FALSE)

head(average_hr_by_day_minute_DayOfWeek)
```

Let's see how long the data set is now:


```{r}
nrow(average_hr_by_day_minute_DayOfWeek)
```
The number of rows looks much more reasonable, and I will try to plot this later. However, I want to create a data set grouped also by hour, in the case this doesn't work.

```{r}
average_hr_by_hour <- average_hr_by_day_minute_DayOfWeek %>%
  separate(Minute, into = c("Hour", "Minute", "Second"), sep = ":") %>%
  group_by(DayOfWeek, Hour) %>%
  summarize(Average_HR = mean(Average_HR))
```
```{r}
write.csv(average_hr_by_hour, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_hr_by_hour.csv", row.names = FALSE)

head(average_hr_by_hour)
```

```{r}
nrow(average_hr_by_hour)
```

It's time to plot our two data sets:
```{r}
library(ggplot2)
```
```{r}
ggplot(average_hr_by_hour, aes(x = Hour, y = Average_HR, fill = DayOfWeek)) +
  geom_col(position = "dodge", width = 0.8, color = "black") +
  labs(x = "Hour", y = "Average Heart Rate", fill = "Day of the Week") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()
```

I won't plot the data set by minute, because there would be too many bars. However, this plot, we can see a trend. Let's make our analysis even simpler by creating a data set that group the Average_HR values in descending order, to see what are the DayOfWeek - Hour combination with the highest values.

```{r}
average_hr_by_hour_sorted <- average_hr_by_hour %>%
  arrange(desc(Average_HR))
```
```{r}
write.csv(average_hr_by_hour_sorted, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/average_hr_by_hour_sorted.csv", row.names = FALSE)

head(average_hr_by_hour_sorted)
```

By looking at this data set, we can already see what hours are the most popular for exercising (of course we need to keep in mind the small sample of participant and the fact that intense physical activity and exceptionally high heart rate values from some participants may skew the results, even if heart rate can go up only so far). Participants may not necessarily being exercising to make their heart rate go up, but they may be doing physical activities of other sort; however, this is not a problem, because physical activity and high a high heart rate still foster a high consumption of body fluids and water.

Let's filer out all of the combination with Average_HR below 85, because we only want to target combinations that represent physical activity.To note that I chose to use the heart rate data set instead of the daily activity data set to track physical activity, because behavioural indicators such as heart rate are more reliable than self-assessed indicators or "steps".

```{r}
active_hours <- average_hr_by_hour_sorted %>%
  filter(Average_HR >= 85)
```

```{r}
write.csv(active_hours, file = "C:/Users/gabri/OneDrive/Desktop/Projects/bellabeat_project/fitbase_data/active_hours.csv", row.names = FALSE)

nrow(active_hours)
```

#Share Phase

The data set is small enough to be fully displayed in a table.
```{r}
View(active_hours)
```

Let's plot it now (I'm using different types of charts and combinations to represent this data set in view it from different angles):

```{r}
ggplot(active_hours, aes(x = reorder(Hour, -Average_HR), y = Average_HR, fill = DayOfWeek)) +
  geom_col(position = position_dodge(width = 1), color = "black") +
  labs(x = "Hour", y = "Average Heart Rate", fill = "Day of the Week") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
ggplot(active_hours, aes(x = Hour, y = Average_HR, color = DayOfWeek)) +
  geom_point(size = 3) +
  labs(x = "Hour", y = "Average Heart Rate", color = "Day of the Week") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
ggplot(active_hours, aes(x = Hour, y = DayOfWeek, size = Average_HR)) +
  geom_point() +
  labs(x = "Hour", y = "Day of the Week", size = "Average Heart Rate") +
  scale_size_continuous(range = c(1, 10)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
ggplot(active_hours, aes(x = paste(DayOfWeek, Hour, sep = " - "), y = Average_HR)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Day of the Week - Hour", y = "Average Heart Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The time slots with the highest Average_HR are:
* Saturdays between 12-13-14
* Wednesdays between 17-18
* Thursdays between 16-17


#Recommendations
Based on the results, and still keeping in mind that the sample was constituted only by 30 individuals, we can recommend a few steps to the marketing team.

Even if the data set was small and exceptionally high heart rate may have skewed the results, heart rate have minimum and maximum possible ranges. Moreover, physical activity is not intended just as exercises but all the behaviours that may have increased the heart rate of the participant, which will in any ways lead to some degree of Dehydration.

This analysis can be also compared with weather seasonal patterns and forecasts to understand at what time individuals will be the most hydrated, even if hot weather may discourage physical activity.

##Results
According to our data, individuals tend to perform the most physical activity in on Saturdays between 12-13-14, Wednesdays between 17-18 (probably after work), and on Thursdays between 16-17. Moreover, also the other day-hour combinations with an heart rate above 85 shouldn't be discarded and should be considered as secondary time slots for physical activity.

##Actions
Bellabeat's marketing team should prefer targeting promotions, communications, and ads for Spring (the smart water bottle) around and especially after the time slots that registered a peak in physical activity, especially Saturdays between 12-13-14, Wednesdays between 17-18, and on Thursdays between 16-17.

Those communications can be ads, organic social media posts, emails, and web or in-app push notifications. Also traditional media such as television and radio can be considered.

Moreover, also other physical activity-related products such as Leaf(tracker) and Time(smart watch).

The next steps I suggest taking is creating a campaign targeted to physical activity and dehydration and test it with organic socials, email, and a low paid ads budget.

Then the campaign will be improved, and if successful, the budget will be expanded for ads and medias such as television and radio will be added to the campaign.



